{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_proper_nouns(text):\n",
    "    tagged_sentence = nltk.tag.pos_tag(text.split())\n",
    "    edited_sentence = [word if tag != 'NNP' and tag != 'NNPS' else \"NNP\" for word,tag in tagged_sentence]\n",
    "    return ' '.join(edited_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(texts):\n",
    "    texts = [remove_proper_nouns(i) for i in texts]\n",
    "    vectorizers = pickle.load(open(\"full-vectorizers.pkl\", \"rb\"))\n",
    "    models = pickle.load(open(\"full-models.pkl\", \"rb\"))\n",
    "    tokens=[[token for token in nltk.tokenize.word_tokenize(text) if token.isalpha()] for text in texts]\n",
    "    tokens=[[token for token in doc if token not in stopwords.words(\"english\")  and token!='NNP'] for doc in tokens]\n",
    "    #tokens=[[porter.stem(token) for token in doc] for doc in tokens]\n",
    "    vec_X={}\n",
    "    for name, vectorizer in vectorizers.items():\n",
    "        vec_X[name]=vectorizer.transform([\" \".join(doc) for doc in tokens])\n",
    "    predictions={}\n",
    "    for name,model in models.items():\n",
    "        print(name, model)\n",
    "        predictions[name]=model.predict(vec_X[name])\n",
    "    return predictions['bow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"movie-plots-test.csv\",index_col=0)\n",
    "test_y=data[\"Genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import classification_report as cr\n",
    "preds=test_model(data[\"Plot\"])\n",
    "cm(test_y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cr(test_y,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "To start the training process, I divide the reviews randomly in 4:1 ratio for a train-validation split to evaluate the training process.\n",
    "\n",
    "Then the train movie reviews have pretty high standard deviation in length so we select those reviews that fall within two standard deviations of the mean in terms of length so as to remove some outliers. This doesn't discard many in any case. \n",
    "\n",
    "The next step is to convert the labels from categorical strings i.e. horror, comedy etc. to integer values. This is done with a simple dictionary but it's important that we retain this map even at inference time so I persistently store the same. \n",
    "\n",
    "Looking at the plots, a vast number of words within them are names of people/places and these add a lot of different words to the vocabulary but don't necessarily convey much additional information. In lieu of this, I use nltk to identify proper nouns within them i.e. POS tag == NNP or NNPS, and then replace all of these with a single NNP token. (I validated with and without this modification and noticed an improvement). This could potentially lose some information but the goal is to build a robust classifier so perhaps it is not so best to learn from such proper nouns because they can lead to some spurious correlations in classification. \n",
    "\n",
    "The next step is to tokenize and filter out stopwords as we had done in the lab. Again NLTK helps here. I also filter out the NNP token here because it is clearly the most common token. \n",
    "\n",
    "I experimented with porter stemming/lemmatization but it didn't obtain an improvement and since we only have ~10^5 examples we can train without this step. \n",
    "\n",
    "Once we have this, we can move on to vectorization. As we had done in the lab I try out three approaches: binary, bag of words and tfidf. For this I used a sparse matrix setup similar to the lab. \n",
    "\n",
    "For classification from these features, I try out multiple models from logistic regression, SVM, Random forest and the Gaussian Naive Bayes and the Gaussian NB was the one that obtained the highest validation accuracy/F1 score so I chose to proceed with this. For each I experimented with a few variations in parameters using the validation set and ultimately went with GNB.\n",
    "\n",
    "Interestingly, the bag of words representation obtained the best performance while tfidf was unable to classify the two minority classes well and the binary approach could not match the bow in performance. \n",
    "\n",
    "We persistently store both the models as well as vectorizers needed to perform classification so that it can be used in inference.\n",
    "\n",
    "### Inference\n",
    "\n",
    "At inference time, we accept a list of strings and labels. First we load the dictionary map for labels from the persistent storage and encode the labels appropriately. Then first we tokenize and remove stopwords/NNP tokens as we had in the train phase. Then we load the persistently stored vectorizers from the train phase to convert the tokens to the vectorized representations. The models are loaded as well and classification is performed to obtain the predictions as a simple array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
