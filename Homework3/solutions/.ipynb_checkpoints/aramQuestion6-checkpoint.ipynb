{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import csv\n",
    "import string\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###First we perform a naive pre-processing step to go from pandas to lists of data\n",
    "###and relabel the genres as 0,1,2,3 to be used with sklearn\n",
    "def preprocess_part1(filename='movie-plots-student.csv'):\n",
    "    data = pd.read_csv(filename)\n",
    "    data.dropna(axis=0,inplace=True)\n",
    "    train_y = []\n",
    "    data_y = data['Genre']\n",
    "    for i in range(len(data_y)):\n",
    "        if data_y[i] == 'comedy':\n",
    "            train_y.append(0)\n",
    "        elif data_y[i] == 'drama':\n",
    "            train_y.append(1)\n",
    "        elif data_y[i] == 'horror':\n",
    "            train_y.append(2)\n",
    "        else: #action\n",
    "            train_y.append(3)\n",
    "    data_X = data['Plot']\n",
    "    for i in range(len(data_X)):\n",
    "        data_X.loc[i] = data_X.loc[i].replace(\"\\r\\n\",\" \")\n",
    "\n",
    "    return data_X, train_y\n",
    "\n",
    "#pre-processing step where we perform lemmatization and stemming; this occurs at the level of a given input\n",
    "def preprocess_part2(text,lemma,stem):\n",
    "    if lemma:\n",
    "        #print('we lemma-ing')\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        text = text.apply(lambda row: \" \".join([lemmatizer.lemmatize(word) for word in row.split(\" \")]))\n",
    "    if stem:\n",
    "        #print('we stemming')\n",
    "        snow = nltk.stem.SnowballStemmer('english')\n",
    "        text = text.apply(lambda row: \" \".join([snow.stem(word) for word in row.split(\" \")]))\n",
    "    return text\n",
    "\n",
    "def preprocess_part3(text, vocab, stopwords_bool,update=False):\n",
    "    # get tokens from file\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    stripped_words = [word for word in stripped if word.isalpha()]\n",
    "    # remove stopwords \n",
    "    if(stopwords_bool):\n",
    "        stop_wordssss = set(stopwords.words('english'))\n",
    "        words = [s for s in stripped_words if not s in stop_wordssss]\n",
    "        # update vocabulary\n",
    "        if update:\n",
    "            vocab.update(words)\n",
    "        # return tokens \n",
    "        return words\n",
    "    else:   \n",
    "        # update vocabulary   \n",
    "        if update:\n",
    "            vocab.update(stripped_words)\n",
    "        # return tokens\n",
    "        return stripped_words\n",
    "\n",
    "class TextClassifier():\n",
    "    # constructor\n",
    "    def __init__(self, model, filename, lemma=False, stem=False, stop_words=False, min_occurence=0):\n",
    "        self.model = model\n",
    "        self.lemma = lemma\n",
    "        self.stem = stem\n",
    "        self.filename = filename\n",
    "        self.min_occurence = min_occurence\n",
    "        self.stop_words = stop_words\n",
    "        data_X, data_Y = preprocess_part1(self.filename)\n",
    "        \n",
    "        data_X = preprocess_part2(data_X,self.lemma,self.stem)    \n",
    "        # preprocess data and build vocabulary\n",
    "        self.vocab = Counter()\n",
    "        data_X = data_X.apply(lambda row: \" \".join(preprocess_part3(row, self.vocab, self.stop_words,update=True)))\n",
    "        # only include words in vocabulary that appear > min_occurence times\n",
    "        self.vocab = [k for k,c in self.vocab.items() if c >= min_occurence]\n",
    "        X = data_X.to_numpy()\n",
    "        Y = np.array(data_Y)\n",
    "        \n",
    "        # train-test split data\n",
    "        self.X_train,self.X_test,self.Y_train,self.Y_test = train_test_split(X,Y, test_size=0.1)\n",
    "        # initialize class variables before fitting\n",
    "        self.best_classifier = None\n",
    "        self.best_score = float('-inf')\n",
    "        self.best_params = None\n",
    "\n",
    "    def vectorize(self, X, params):\n",
    "        # vectorize data according to vocabulary \n",
    "        vectorizer = CountVectorizer(vocabulary=self.vocab, **params)\n",
    "        return vectorizer.fit_transform(X)\n",
    "\n",
    "    def fit(self,params):\n",
    "        vectorized_X = self.vectorize(self.X_train, params)\n",
    "        score = cross_val_score(self.model, vectorized_X,self.Y_train, cv=5).mean()\n",
    "        # update best score seen from 5-folds of cross validation \n",
    "        if (score > self.best_score):\n",
    "            self.best_classifier = self.model.fit(vectorized_X,self.Y_train)\n",
    "            self.best_score = score\n",
    "            self.best_params = params\n",
    "       \n",
    "    # function to get accuracy of model \n",
    "    def test_set_score(self):\n",
    "        vectorized_X_test = self.vectorize(self.X_test, self.best_params)\n",
    "        predictions = self.best_classifier.predict(vectorized_X_test)\n",
    "        # print accuracy score of predictions \n",
    "        print('Accuracy on 10 percent of hold-out data:', accuracy_score(self.Y_test,predictions))\n",
    "\n",
    "    def test_model(self,test_data_name):\n",
    "        ###expects the NAME of the csv file\n",
    "        #re-perform all preprocessing steps, but without updating the vocabulary!\n",
    "        testdata_X, testdata_Y = preprocess_part1(test_data_name)\n",
    "        testdata_X = preprocess_part2(testdata_X,self.lemma,self.stem)\n",
    "        testdata_X = testdata_X.apply(lambda row: \" \".join(preprocess_part3(row, self.vocab, \n",
    "                                                                    self.stop_words,update=False)))\n",
    "        \n",
    "        testdataX_np = testdata_X.to_numpy()\n",
    "        testdataY_np = np.array(testdata_Y)\n",
    "        \n",
    "        #vectorize the data\n",
    "        testdataX_vec = self.vectorize(testdataX_np, self.best_params)\n",
    "        #get predictions from the best classifier\n",
    "        predictions_test = self.best_classifier.predict(testdataX_vec)\n",
    "        \n",
    "        print('Test set accuracy:', accuracy_score(testdataY_np,predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time/my poor computing power on my laptop, I restricted my attention to model training with at most 500 iterations. I found that when I do not use stop-words, I needed to increase the number of iterations to achieve convergence, so I decided to always use stop-words for the remainder of the experiments below. Finally, there's another variable to remove frequency of words from the vocabulary, but I didn't get around to running different tests for those variables, so it's always set to 0.\n",
    "\n",
    "I tried various combinations of lemmatizing/stemming/regularization with the Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10 percent of hold-out data: 0.7098880597014925\n",
      "Test set accuracy: 0.9706980216498694\n"
     ]
    }
   ],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=5, max_iter=500), filename='movie-plots-student.csv', lemma=False, stem=False, stop_words=True, min_occurence=0)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('movie-plots-student.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10 percent of hold-out data: 0.710820895522388\n",
      "Test set accuracy: 0.9704180664427025\n"
     ]
    }
   ],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=5, max_iter=500), filename='movie-plots-student.csv', lemma=True, stem=True, stop_words=True, min_occurence=0)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('movie-plots-student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10 percent of hold-out data: 0.7042910447761194\n",
      "Test set accuracy: 0.883631952220978\n"
     ]
    }
   ],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=0.01, max_iter=500), filename='movie-plots-student.csv', lemma=False, stem=False, stop_words=True, min_occurence=0)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('movie-plots-student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10 percent of hold-out data: 0.7145522388059702\n",
      "Test set accuracy: 0.8774729376633073\n"
     ]
    }
   ],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=0.01, max_iter=500), filename='movie-plots-student.csv', lemma=True, stem=True, stop_words=True, min_occurence=0)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('movie-plots-student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10 percent of hold-out data: 0.7052238805970149\n",
      "Test set accuracy: 0.954273982829414\n"
     ]
    }
   ],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=0.1, max_iter=500), filename='movie-plots-student.csv', lemma=True, stem=True, stop_words=True, min_occurence=0)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('movie-plots-student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10 percent of hold-out data: 0.7257462686567164\n",
      "Test set accuracy: 0.9606196341918626\n"
     ]
    }
   ],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=0.1, max_iter=500), filename='movie-plots-student.csv', lemma=False, stem=False, stop_words=True, min_occurence=0)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('movie-plots-student.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the best model is when Lemmatizing is True, Stemming is True, and regularization parameter is set to 5 (note that this is close to the case where Lemmatizing and Stemming are False). For the final test model, please execute the following (with 'insert name here' replaced with the new csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = TextClassifier(LogisticRegression(C=5, max_iter=500), filename='movie-plots-student.csv', lemma=True, stem=True, stop_words=True, min_occurence=2)\n",
    "lg.fit({\"ngram_range\":(1,1)})\n",
    "lg.test_set_score()\n",
    "lg.test_model('insert name here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
