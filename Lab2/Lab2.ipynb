{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science, Lab 2 (9/23)\n",
    "- Processing data (+ Google Trends)\n",
    "- Classification with Decision Trees, Random Forest, Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presidential Elections 2016\n",
    "We will model the results of 2016 deferal elections in each individual state based on the Google Trends data for search expressions of our choice. In this classification problem, the target variable $Y_i$ indicates if Trump received more popular votes than any other candidate in state $i$. The election results by state are contained in file *electoral_data.csv*, which we will need to process for our needs.  First, however, let's figure out how pyTrends work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "pt=TrendReq(hl='en-US',retries=10) # host language, retries.\n",
    "\n",
    "# Google trends normalizes all trends data between 0 and 100 and accounts for\n",
    "# the number of total searches at a given time and location. Google trends releases\n",
    "# only relative interest magnitudes.\n",
    "\n",
    "# Example:\n",
    "search_words=[\"clinton\",\"trump\",\"pandemic\"] # terms of interest\n",
    "pt.build_payload(search_words,timeframe='2016-10-31 2016-11-07',geo='US') # temporal and spatial scope;\n",
    "data=pt.interest_by_region(resolution='REGION') # 'REGION' flag will give the by-state statstics;\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each region (state) the sum is 100, distributed among search expressions. Is this what we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to decide on:\n",
    "#  - Search expressions whose trends to be used as regressors;\n",
    "#  - Temporal scope of the fetched trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Preparing the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the electoral data:\n",
    "data=pd.read_csv('electoral_data.csv',header=1,sep=';',error_bad_lines=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping extra columns and rows:\n",
    "data.drop(data.columns[[1,2,-3,-2,-1]],axis=1,inplace=True)\n",
    "data.drop(data.index[51:59],inplace=True)\n",
    "\n",
    "# Rename columns:\n",
    "new_columns=['state','trumpRes','clintonRes']\n",
    "data.rename(columns={name:new_columns[i] for i,name in enumerate(data.columns)},inplace=True)\n",
    "\n",
    "# Rename rows by states:\n",
    "new_index={ind:data.loc[i,\"state\"] for i,ind in enumerate(data.index)}\n",
    "data=data.rename(index=new_index)\n",
    "\n",
    "# Remove states column:\n",
    "data.drop(\"state\",axis=1,inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check there are no Nans:\n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There is one hidden problem:\n",
    "display(data.loc[:,'trumpRes'].iloc[0:5]) # seems ok;\n",
    "print(type(data.loc[:,'trumpRes'].iloc[0])) # entries are actually strings...\n",
    "display(data.loc[:,'trumpRes'].iloc[0]) # '\\xa0' is a code for non-breaking space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fix this:\n",
    "data.loc[:,'trumpRes']=data.loc[:,'trumpRes'].apply(lambda x:  x.replace(u'\\xa0','')).astype(int) # 'u' stands for unicode\n",
    "data.loc[:,'clintonRes']=data.loc[:,'clintonRes'].apply(lambda x:  x.replace(u'\\xa0','')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check:\n",
    "print(type(data.loc[:,'trumpRes'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data from PyTrends:\n",
    "\n",
    "for sw in search_words:\n",
    "    pt.build_payload(sw,timeframe=scope,geo='US')\n",
    "    data[sw]=pt.interest_by_region(resolution='REGION').values # values needed not to worry about index mismatch\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if PyTrends fail:\n",
    "\n",
    "# data=pd.read_csv('clean_data.csv',header=0,index_col=0)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute votes proportion:\n",
    "data['trumpWinner']=data.loc[:,'trumpRes']>data.loc[:,'clintonRes'].astype(int)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop more unnecessary columns but save \"trumpRes\" separately (for future):\n",
    "trumpMargin=pd.Series(((data.loc[:,\"trumpRes\"]-data.loc[:,\"clintonRes\"])/(data.loc[:,\"trumpRes\"]+data.loc[:,\"clintonRes\"])).apply(lambda x: round(x,3)),index=data.index,name=\"trumpMargin\")\n",
    "data.drop([\"trumpRes\",\"clintonRes\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split:\n",
    "import numpy as np\n",
    "train_ind=np.random.choice(range(len(data)),size=int(0.75*len(data)),replace=False)\n",
    "test_ind=np.delete(np.arange(len(data)),train_ind)\n",
    "train=data.iloc[train_ind]\n",
    "test=data.iloc[test_ind]\n",
    "print(f'Positive labels in train: {sum(train.loc[:,\"trumpWinner\"])} out of {len(train)} ({100*sum(train.loc[:,\"trumpWinner\"])/len(train):.0f}%)')\n",
    "print(f'Positive labels in test: {sum(test.loc[:,\"trumpWinner\"])} out of {len(test)} ({100*sum(test.loc[:,\"trumpWinner\"])/len(test):.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to standardize features for Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model 1: A single Decision Tree*\n",
    "Decision Trees are non-parametric non-linear models for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "model_1=DTC(criterion='entropy',min_samples_split=5)\n",
    "model_1.fit(train.iloc[:,:-1],train.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree:\n",
    "import graphviz\n",
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz as export\n",
    "dot_format=export(model_1,class_names=[\"Clinton\",\"Trump\"],filled=True,feature_names=train.columns[:-1])\n",
    "pydot_graph=pydotplus.graph_from_dot_data(dot_format)\n",
    "pydot_graph.set_size('\"4,4!\"')\n",
    "graphviz.Source(pydot_graph.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's color the nodes correctly (red for Trump and blue for Clinton).\n",
    "# Object 'pydot_graph' has nodes as attributes, which are described as:\n",
    "pydot_graph.get_node_list()[1].get_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: parse strings of this format to get the list of 'nvalues';\n",
    "# Example: [17,21] should be the result of parsing the above string\n",
    "# Hint: one built-in method from the last lab can be especially useful.\n",
    "\n",
    "def parse(label):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in pydot_graph.get_node_list():\n",
    "    if node.get_label(): # the first node is 'None' \n",
    "        clinton,trump=parse(node.get_label())\n",
    "        if clinton>trump:\n",
    "            color='#{a:02x}{a:02x}ff'.format(a=int(2*255*trump/(trump+clinton)))\n",
    "        else:\n",
    "            color='#ff{a:02x}{a:02x}'.format(a=int(2*255*clinton/(trump+clinton)))\n",
    "        node.set_fillcolor(color)\n",
    "graphviz.Source(pydot_graph.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch predictions on the test set:\n",
    "\n",
    "preds=model_1.predict(test.iloc[:,:-1])\n",
    "results=[[int(p),int(t),int(not p^t)] for p,t in zip(preds,test.iloc[:,-1])]\n",
    "results_df=pd.DataFrame(results,columns=['predicted','truth','correct?'],index=test.index)\n",
    "display(results_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=np.sum([not p^t for p,t in zip(preds,test.iloc[:,-1])]) # ^ is logical xor\n",
    "print(f'correct test predictions: {correct} out of {len(test)} ({100*correct/len(test):.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature Importance*\n",
    "Internal nodes of Decision Trees always split based on a single feature and parallel to all other features. The split feature is chosen as to yield maximum reduction in the total weighted loss (e.g. entropy for classification and mean squared error for regression) after the split. Thus, it becomes possible to quantiy the roles (i.e importance) of each regressor by looking at the reduction in loss (if any) it brought across all splits in the tree. For example, this ranking procedure is realized as the \"*feature_importances_*\" attribute of a *DecisionTree* object in *scikit-learn*. Before fetching feature importance from our tree model, let's understand how exactly *scikit-learn* computes it.\n",
    "\n",
    "Let $N$ be the set of all internal (non-leaf) nodes. For each $n\\in N$, let $s_n$ be the number of samples inside it and $s_n^{(l)}$, $s_n^{(r)}$ be the counts of samples inside its left and right children, respectively. Similarly, let $c_n$ be the impurity of the node $n$ and $c_n^{(l)}$, $c_n^{(r)}$ be node impurities of its left and right children, respectively. Denote the set of nodes that split based on feature $f$ by $N_{f}$.  Then, the unnormalized importance score $M_f$ of feature $f$ is compted as \n",
    "\n",
    "$$M_f=\\left(\\sum_\\limits{n\\in N_f}s_nc_n-s_n^{(l)}c_n^{(l)}-s_n^{(r)}c_n^{(r)}\\biggr/\\sum_\\limits{n\\in N}s_nc_n-s_n^{(l)}c_n^{(l)}-s_n^{(r)}c_n^{(r)}\\right).$$\n",
    "\n",
    "The normalized score is then computed by dividing $M_f$ by the sum of importance scores across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sw,imp in zip(train.columns[:-1],model_1.feature_importances_):\n",
    "    print(f'the importance score for \"{sw}\" is {imp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.barh(range(len(train.columns[:-1])),sorted(model_1.feature_importances_),color=\"lightskyblue\",align=\"center\")\n",
    "plt.yticks(range(len(train.columns[:-1])),train.columns[:-1][model_1.feature_importances_.argsort()])\n",
    "plt.xlabel('importance')\n",
    "plt.xlim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model 2: Random Forest*\n",
    "Random Forest, as the name suggests, consists of fitting multiple decision trees on (randomly) different datasets and averaging their outputs at inference. This procedure is an instance of a subset of ensebling methods called \"bagging\" (bootstrap aggregating). In bagging, one creates $n$ datasets by randomly sampling (with replacement) from the actual \"master\" dataset and then fitting $n$ separate learners (e.g., trees), averaging them together during inference. Note that learners are indeed \"separate\" (with no interdepedence) and can train concurrently. In practice, this works better and more consistently compared to single-learner models by reducing variance of predictions. Moreover, with bagging, it is possible to estimate uncertainty of pedictions by computing their variance across all learners.\n",
    "\n",
    "Random Forests is a classic example of baggig technique. However, in addition to the above procedure, each candidate split in any given learner is only possible on a feature from a randomly selected subset of features (of size $\\sqrt{d}$ in *sklearn* by default) which is resampled after evaluation (i.e. not shared between candidate splits). This step encourages indiviudal learners to become different, hence increasing the variability across learners and making the resulting ensemble model more robust, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "num_learners=200\n",
    "model_2=RFC(n_estimators=num_learners,criterion='entropy',min_samples_split=5,bootstrap=True,max_features=None)\n",
    "model_2.fit(train.iloc[:,:-1],train.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract individual trees:\n",
    "print(f'we fitted {len(model_2.estimators_)} decision trees!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize individual trees. Note that the class balance in the root node is different from tree to tree (because we set *bootstrap=True*, which makes train samples for each tree to be randomly sampled from our overall train data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trees one by one:\n",
    "tree=model_2.estimators_[54] # change this!\n",
    "dot_format=export(tree,class_names=[\"Clinton\",\"Trump\"],filled=True,feature_names=train.columns[:-1])\n",
    "pydot_graph=pydotplus.graph_from_dot_data(dot_format)\n",
    "pydot_graph.set_size('\"4,4!\"')\n",
    "for node in pydot_graph.get_node_list():\n",
    "    if node.get_label(): # the first node is 'None' \n",
    "        clinton,trump=parse(node.get_label())\n",
    "        if clinton>trump:\n",
    "            color='#{a:02x}{a:02x}ff'.format(a=int(2*255*trump/(trump+clinton)))\n",
    "        else:\n",
    "            color='#ff{a:02x}{a:02x}'.format(a=int(2*255*clinton/(trump+clinton)))\n",
    "        node.set_fillcolor(color)\n",
    "graphviz.Source(pydot_graph.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch predictions on the test set:\n",
    "\n",
    "preds=model_2.predict(test.iloc[:,:-1])\n",
    "results=[[int(p),int(t),int(not p^t)] for p,t in zip(preds,test.iloc[:,-1])]\n",
    "results_df=pd.DataFrame(results,columns=['predicted','truth','correct?'],index=test.index)\n",
    "display(results_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy:\n",
    "correct=np.sum([not p^t for p,t in zip(preds,test.iloc[:,-1])]) # ^ is logical xor\n",
    "print(f'correct test predictions: {correct} out of {len(test)} ({100*correct/len(test):.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the computation of feature importance from a single decision tree. Can we generalize to random forests? What could be the simplest extension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance:\n",
    "plt.barh(range(len(train.columns[:-1])),sorted(model_2.feature_importances_),color=\"lightskyblue\",align=\"center\")\n",
    "plt.yticks(range(len(train.columns[:-1])),train.columns[:-1][model_2.feature_importances_.argsort()])\n",
    "plt.xlabel('importance')\n",
    "plt.xlim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the uncertainty of the predictions on $x$ made by individual trees by one standard deviation: $\\hat{\\sigma}(x)=\\sqrt{\\frac{\\sum_{i=1}^{B}\\left(\\hat{y_i}(x)-y(x)\\right)^2}{B-1}}$, where $\\hat{y_i}(x)$ is predicted class of input $x$, $y(x)$ is ground truth for $x$ and $B$ is the number of learners in our ensemple ($B=200$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate uncertainty of predictions:\n",
    "predictions=[]\n",
    "for tree in model_2.estimators_:\n",
    "    predictions.append(tree.predict(test.iloc[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty(x,y,y_hat):\n",
    "    y_hat_x=[prediction[x] for prediction in y_hat]\n",
    "    sigma_hat=np.sqrt(np.divide(sum([(p-y[x])**2 for p in y_hat_x]),num_learners))\n",
    "    return sigma_hat\n",
    "uncertainty(7,test.iloc[:,-1],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: estimate uncertainty (\"difficulty\") of different test observations (states);\n",
    "# In particular, create and display a pandas DataFrame showing uncertainty for each of 12 states.\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some states seem to be \"easy\" and correctly classified by nearly all trees; some others are remarkably hard. How can we get a better understanding of why some states are harder than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulties=difficulties.T\n",
    "difficulties=difficulties.append(trumpMargin[test.index])\n",
    "display(difficulties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=0.01\n",
    "plt.scatter(x=difficulties.loc[\"sigma_hat\"],y=difficulties.loc[\"trumpMargin\"].apply(lambda x: abs(x)),c='fuchsia')\n",
    "for i,col in enumerate(difficulties.columns):\n",
    "    plt.annotate(col,(difficulties.loc[\"sigma_hat\"].iloc[i]+offset,abs(difficulties.loc[\"trumpMargin\"].iloc[i])+offset))\n",
    "plt.xlabel(\"difficulty\")\n",
    "plt.ylabel(\"trumpMargin\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make learners focus on \"difficult\" observations (states)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model 3: Gradient Tree Boosting*\n",
    "Just as Random Forest is an example of bagging, Gradient Tree Boosting is an instance of yet another type of ensembling called Boosting. The philosophy behind these methods is to train multiple learners *in sequence* and focus each new learner on improving the *overall* performance of the current ensemble. Thus, in particular, each new learner must concentrate on most \"difficult\" examples (those that the current ensemble mispredicts) from the training data. In practice, this \"focus\" can be enforced in many forms either implicitly of explicitly. For example, one of the simplest approaches is to increase the weight of difficult training examples in the loss function of a new learner. Alternatively, the sampling probability of a training example could be linked to its current \"difficulty\", resulting in drawing troublesome data more often. Inference in boosting algorithms is weighted, i.e. predictions of different learners are not considered equally valuable.\n",
    "\n",
    "Gradient Boosting shares the same ideology as described above, however, the \"focus\" on difficult examples in subsequent learners is enforced implicitly. New learners are fitted to (i.e. having as a target variable) the so-called *pseudo-residuals* of the current ensemble model (more on this in DS-GA 1003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "model_3=GBC(n_estimators=200,min_samples_split=5,max_features=None)\n",
    "model_3.fit(train.iloc[:,:-1],train.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch predictions on the test set:\n",
    "\n",
    "preds=model_3.predict(test.iloc[:,:-1])\n",
    "results=[[int(p),int(t),int(not p^t)] for p,t in zip(preds,test.iloc[:,-1])]\n",
    "results_df=pd.DataFrame(results,columns=['predicted','truth','correct?'],index=test.index)\n",
    "display(results_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=np.sum([not p^t for p,t in zip(preds,test.iloc[:,-1])]) # ^ is logical xor\n",
    "print(f'correct test predictions: {correct} out of {len(test)} ({100*correct/len(test):.0f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
