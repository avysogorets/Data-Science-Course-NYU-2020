{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Introduction to Data Science, Lab 3 (9/30)\n",
    "- Statistical Learning Theory;\n",
    "- Logistic regression from scratch.\n",
    "\n",
    "## SLT: Risk and Error Decomposition\n",
    "#### Statistical Learning framework\n",
    "- *Input* space: $\\mathcal{X}$ (range of $X$), *output* space: $\\mathcal{Y}$ (range of $Y$), *action* space: $\\mathcal{A}$;\n",
    "- *Generating (data) distribution*: $(X,Y)\\sim P_{X,Y}$ over $\\mathcal{X}\\times\\mathcal{Y}$;\n",
    "- *Decision function*: $f\\colon\\mathcal{X}\\rightarrow\\mathcal{A}$ (i.e., *the model*); *hypothesis space* $\\mathcal{F}$ (an part of *inductive bias*);\n",
    "- *Evaluation function* (loss function): $\\mathcal{L}\\colon\\mathcal{A}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ (e.g., $\\mathcal{L}(a,y)=(a-y)^2-$squared error);\n",
    "- *The statistical risk* of a decision function (expected loss): $\\mathcal{R}(f)=\\mathbb{E}_P\\mathcal{l}(f(x),y)$;\n",
    "- *Bayes decision function*: $f^{*}=\\text{argmin}_f\\mathcal{R}(f)$ (statistical risk minimizer);\n",
    "- *Risk minimizer in $\\mathcal{F}$*: $f_{\\mathcal{F}}=\\text{argmin}_{f\\in\\mathcal{F}}\\mathcal{R}(f)=\\text{argmin}_{f\\in\\mathcal{F}}\\mathbb{E}_P\\mathcal{L}(f(X),Y)$;\n",
    "- Given data $\\mathcal{D}=\\{(x_i,y_i)\\}_{i=1}^{n}$, *empirical risk*: $\\hat{\\mathcal{R}}_{\\mathcal{D}}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(f(x_i),y_i)$;\n",
    "- *Empirical risk minimizer (ERM) in $\\mathcal{F}$*: $\\hat{f}_{\\mathcal{F},\\mathcal{D}}=\\text{argmin}_{f\\in\\mathcal{F}}\\hat{\\mathcal{R}}_{\\mathcal{D}}(f)=\\text{argmin}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(f(x_i),y_i)$; does it always exist?\n",
    "- *Excess risk* of a function $f$: $E(f)=\\mathcal{R}(f)-\\mathcal{R}(f^{*})$ (can $\\mathcal{E}(f)$ be negative?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Consider input/output/action spaces be $\\mathcal{X}=\\mathcal{Y}=\\mathcal{A}=\\mathbb{Z}$, generating (data) distribution $P_{X\\times Y}$ to be unifom on $\\{(-2,0),(0,0),(0,2),(2,1),(2,2)\\}$, hypothesis space $\\mathcal{F}=\\{\\alpha x+\\beta\\colon \\alpha,\\beta\\in\\mathbb{R}\\}$ of linear functions on $\\mathbb{R}$, loss function $\\mathcal{L}(a,y)=|a-y|$. What is the Bayes decision function $f^{*}$ and its risk (called *Bayes risk*) $\\mathcal{R}(f^{*})$? What $\\mathcal{F}$ will have a smaller/larger Bayes risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot([-2,2.5],[0,1.125],linestyle='-',color='grey',lw=2.5)\n",
    "#plt.plot([-2,2.5],[0,2.25],linestyle='-',color='grey',lw=2.5)\n",
    "#plt.fill_between(x=[-2,2.5],y1=[0,1.125],y2=[0,2.25],alpha=0.3,color=\"grey\")\n",
    "plt.scatter([-2,0,0,2,2],[0,0,2,1,2],c=\"fuchsia\",s=100)\n",
    "\n",
    "#[plt.plot([0,1-i*0.02],[0,2.5],linestyle='-',color='grey',lw=0.5-0.01*i) for i in range(50)]\n",
    "#plt.scatter([0,0],[0,2],c=\"turquoise\",s=100)\n",
    "\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.ylim((-1,3))\n",
    "plt.xlim((-3,3))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "#### Excess Risk Decomposition\n",
    "The excess risk of an ERM minimizer in $\\mathcal{F}$ given data $\\mathcal{D}$ can be decomposed: $E(\\hat{f}_{\\mathcal{F},\\mathcal{D}})=\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f^{*})=\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f_{\\mathcal{F}})+\\mathcal{R}(f_{\\mathcal{F}})-\\mathcal{R}(f^{*})$.\n",
    "- *Estimation error*: $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}=\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f_{\\mathcal{F}})$ (is it always non-positive, always non-negative, or neither?);\n",
    "- *Approximation error*: $\\mathcal{E}_{\\mathcal{F}}=\\mathcal{R}(f_{\\mathcal{F}})-\\mathcal{R}(f^{*})$ (is it always non-positive, always non-negative, or neither?),\n",
    "\n",
    "so that $E(\\hat{f}_{\\mathcal{F},\\mathcal{D}})=\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}+\\mathcal{E}_{\\mathcal{F}}$. The modelling task in a nutshell: (a) Choose a loss function $\\mathcal{L}$, (b) choose a hypothesis space $\\mathcal{F}$, (c) optimize the empirical risk $\\mathcal{\\hat{R}}_{\\mathcal{D}}(f)$ over $\\mathcal{F}$. Do you always expect (or want) to get an ERM minimizer $\\hat{f}_{\\mathcal{F},\\mathcal{D}}$ as a result? Why or why not?\n",
    "\n",
    "In practice, we will get some $\\tilde{f}_{\\mathcal{F},\\mathcal{D}}$ as a result of optimization. We thus can define *optimization error* $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}=\\mathcal{R}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\mathcal{R}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})$. Hence the excess risk of the resulting model $\\tilde{f}_{\\mathcal{F},\\mathcal{D}}$ is\n",
    "\n",
    "$$E(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})=\\mathcal{R}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\mathcal{R}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})+\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f_{\\mathcal{F}})+\\mathcal{R}(f_{\\mathcal{F}})-\\mathcal{R}(f^{*})=\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}+\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}+\\mathcal{E}_{\\mathcal{F}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand $\\tilde{f}_{\\mathcal{F},\\mathcal{D}}$ and optimization error $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}$ better:\n",
    "1. Is optimization error $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}=\\mathcal{R}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\mathcal{R}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})$ always non-positive, always non-negative, or neither?\n",
    "2. Is quantity $\\hat{\\mathcal{R}}_{\\mathcal{D}}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\hat{\\mathcal{R}}_{\\mathcal{D}}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})$ always non-positive, always non-negative, or neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Exercises\n",
    "1. Let $\\mathcal{F}_1\\subseteq\\mathcal{F}_2$ be two hypothesis spaces. Is the difference of approximation errors $\\mathcal{E}_{\\mathcal{F}_1}-\\mathcal{E}_{\\mathcal{F}_2}$ always non-positive, always non-negative, or neither?\n",
    "2. Let $\\mathcal{F}$ be some hypothesis space and $\\mathcal{D}_1,\\mathcal{D}_2$ be two datasets generated by $P_{X\\times Y}$ with $|\\mathcal{D}_1|<|\\mathcal{D}_2|$. Is $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}_1}-\\mathcal{E}_{\\mathcal{F},\\mathcal{D}_2}$ always non-positive, always non-negative, or neither? What about $\\mathcal{E}^{(o)}_{\\mathcal{F},\\mathcal{D}_1}-\\mathcal{E}^{(o)}_{\\mathcal{F},\\mathcal{D}_2}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation\n",
    "From a probabilistic perspective, logistic regression is a parametric predictive model $Y\\sim Bernoulli(\\sigma(\\theta^T X))$, where $\\sigma=(1+e^{-\\theta^T X})^{-1}$ is a logistic function applied to $\\theta^T X$ where $\\theta$ is a parameter vector of the model. \n",
    "\n",
    "#### Theory of fitting\n",
    "The likelihood $\\mathcal{L}$ and negative log-likelihood $l$ functions of this model are\n",
    "\n",
    "$$\\mathcal{L}(\\theta|Y)=p(y|X,\\theta)=\\prod_{i=1}^{n}\\sigma\\left(\\theta^T \\bar{x_i}\\right)^{y_i}\\left(1-\\sigma\\left(\\theta^T\\bar{x_i}\\right)\\right)^{1-y_i},$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}y_i\\log\\sigma\\left(\\theta^T\\bar{x}_i\\right)+(1-y_i)\\log\\left(1-\\sigma\\left(\\theta^T \\bar{x}_i\\right)\\right),$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}y_i\\log\\left(1+e^{\\theta^T\\bar{x}_i}\\right)^{-1}+(1-y_i)\\log\\left(\\frac{e^{\\theta^T \\bar{x}_i}}{1+e^{\\theta^T\\bar{x}_i}}\\right),$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}-y_i\\log\\left(1+e^{\\theta^T\\bar{x}_i}\\right)+\\theta^T\\bar{x}_i-y_i\\theta^T\\bar{x}_i-\\log(1+e^{\\theta^T\\bar{x}_i})+y_i\\log\\left(1+e^{\\theta^T\\bar{x}_i}\\right)$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}\\theta^T\\bar{x}_i-y_i\\theta^T\\bar{x}_i-\\log(1+e^{\\theta^T\\bar{x}_i})$$\n",
    "\n",
    "\n",
    "The loss fuction above is called *binary cross-entropy*; it is frequently used for classification. We will use gradient descent on the likelihood function to obtain an estimate $\\hat{\\theta}$. Therefore, we need to compute partial derivatives\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\theta_j}=-\\sum_{i=1}^{n}x_{ij}-y_{i}x_{ij}-\\frac{x_{ij}e^{\\theta^T\\bar{x}_i}}{1+e^{\\theta^T\\bar{x}_i}}=-\\sum_{i=1}^{n}x_{ij}\\left(-y_{i}+\\sigma\\left(\\theta^T\\bar{x}_i\\right)\\right)=\\sum_{i=1}^{n}x_{ij}y_{i}-x_{ij}\\sigma\\left(\\theta^T\\bar{x}_i\\right)$$\n",
    "\n",
    "Now, let's fugure out the update rules. Letting $\\alpha$ be the learning rate, we have\n",
    "\n",
    "$$\\theta^{(t+1)}_j=\\theta^{(t)}_j-\\alpha\\frac{\\partial l}{\\partial\\theta_{j}}\\biggr|_{\\theta=\\theta^{(t)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory of predicting\n",
    "Given input vector $\\bar{x}$, the output of logistic regression is a number $\\sigma\\left(\\theta\n",
    "^T\\bar{x}\\right)\\in[0,1]$. To convert it into a prediction $y\\in\\{0,1\\}$, we need to agree on the mapping $\\sigma\\left(\\theta^T\\bar{x}\\right)\\mapsto y$. By default, the threshold value is naturally set to $\\tau=0.5$, i.e. $y=\\mathbb{1}_{\\sigma\\left(\\theta^T\\bar{x}\\right)\\geq 0.5}$. The choice of $\\tau$ affects the trade-off between type I/II errors; $\\tau=0.5$ is the unbiased choice, where we don't differentiate between error types. However, in some applications, costs associated with type I/II errors might be surprisingly different (can you think of such scenarios)?  Therefore, it is common to consider other [biased] threshold values. In fact, one common measure of model's performance is *AUC ROC (Area Under the Receiver Operating Curve)*, which evaluates the model at all thresholds simultaneously (more later in this course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# We will reuse the voting data from last time:\n",
    "train=pd.read_csv('clean_train.csv',sep=',',index_col=0)\n",
    "test=pd.read_csv('clean_test.csv',sep=',',index_col=0)\n",
    "train_X=train.iloc[:,:-1]\n",
    "train_y=train.iloc[:,-1].astype(int)\n",
    "test_X=test.iloc[:,:-1]\n",
    "test_y=pd.DataFrame(test.iloc[:,-1].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to avoid sigmoid saturation:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "train_X=scaler.fit_transform(train_X)\n",
    "test_X=scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions:\n",
    "def sigmoid(t):\n",
    "    return 1./(1+math.e**t)\n",
    "\n",
    "def compute_loss(X,y,theta):\n",
    "    return -sum([y_i*np.log(sigmoid(np.dot(theta,X_i)))+(1-y_i)*np.log(1-sigmoid(np.dot(theta,X_i))) for X_i,y_i in zip(X,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LogisticRegression class\n",
    "class LogisticRegression():\n",
    "    def __init__(self,alpha,max_iter,tolerance=1e-5):\n",
    "        self.alpha=alpha\n",
    "        self.tolerance=tolerance\n",
    "        self.max_iter=max_iter\n",
    "    def gradient(self,X,y,theta):\n",
    "        return [sum([X_i[j]*y_i-X_i[j]*sigmoid(np.dot(X_i,theta)) for X_i,y_i in zip(X,y)]) for j in range(X.shape[1])]\n",
    "    def fit(self,X,y):\n",
    "        progress=[]\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        self.theta=np.random.normal(loc=0,scale=1,size=X.shape[1])\n",
    "        loss=compute_loss(X,y,self.theta)\n",
    "        while self.max_iter>0 and loss>self.tolerance:\n",
    "            if self.max_iter%50==0:\n",
    "                progress.append(loss)\n",
    "            self.theta-=self.alpha*np.array(self.gradient(X,y,self.theta))\n",
    "            loss=compute_loss(X,y,self.theta)\n",
    "            self.max_iter-=1\n",
    "        return progress\n",
    "    def predict(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [1 if sigmoid(np.dot(self.theta,X_i))>0.5 else 0 for X_i in X]\n",
    "    def proba(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [sigmoid(np.dot(self.theta,X_i)) for X_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model:\n",
    "model=LogisticRegression(0.01,1000)\n",
    "progress=model.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IN</th>\n",
       "      <th>ME</th>\n",
       "      <th>MI</th>\n",
       "      <th>MO</th>\n",
       "      <th>MT</th>\n",
       "      <th>NV</th>\n",
       "      <th>NH</th>\n",
       "      <th>NJ</th>\n",
       "      <th>OR</th>\n",
       "      <th>PA</th>\n",
       "      <th>RI</th>\n",
       "      <th>SC</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trumpWinner</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct?</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             IN  ME  MI  MO  MT  NV  NH  NJ  OR  PA  RI  SC  SD\n",
       "trumpWinner   1   0   1   1   1   0   0   0   0   1   0   1   1\n",
       "predicted     1   1   1   1   1   1   0   0   0   1   0   1   1\n",
       "correct?      1   0   1   1   1   0   1   1   1   1   1   1   1"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch predictions:\n",
    "test_y[\"predicted\"]=model.predict(test_X)\n",
    "test_y[\"correct?\"]=(test_y[\"predicted\"]==test_y[\"trumpWinner\"]).astype(int)\n",
    "test_y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we assessed the difficulty of a particular state (observation) by calculating the variance of the associated predictions across all learners in the ensemble. Can we get similar insights from our logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IN</th>\n",
       "      <th>ME</th>\n",
       "      <th>MI</th>\n",
       "      <th>MO</th>\n",
       "      <th>MT</th>\n",
       "      <th>NV</th>\n",
       "      <th>NH</th>\n",
       "      <th>NJ</th>\n",
       "      <th>OR</th>\n",
       "      <th>PA</th>\n",
       "      <th>RI</th>\n",
       "      <th>SC</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trumpWinner</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted</th>\n",
       "      <td>0.953</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct?</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                IN     ME     MI     MO     MT     NV     NH     NJ     OR  \\\n",
       "trumpWinner      1      0      1      1      1      0      0      0      0   \n",
       "predicted    0.953  0.839  0.685  0.971  0.647  0.965  0.169  0.033  0.313   \n",
       "correct?         1      0      1      1      1      0      1      1      1   \n",
       "\n",
       "                PA     RI     SC     SD  \n",
       "trumpWinner      1      0      1      1  \n",
       "predicted    0.709  0.076  0.891  0.999  \n",
       "correct?         1      1      1      1  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the actual \"probabilities\" (sigmoid output):\n",
    "test_y=pd.DataFrame(test_y,dtype=object) # otherwise Pandas will convert all ints to floats\n",
    "test_y[\"predicted\"]=[round(p,3) for p in model.proba(test_X)]\n",
    "test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "direction": "ltr",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
