{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Introduction to Data Science, Lab 3 (9/30)\n",
    "- Statistical Learning Theory;\n",
    "- Logistic regression from scratch.\n",
    "\n",
    "## SLT: Risk and Error Decomposition\n",
    "#### Statistical Learning framework\n",
    "- *Input* space: $\\mathcal{X}$ (range of $X$), *output* space: $\\mathcal{Y}$ (range of $Y$), *action* space: $\\mathcal{A}$;\n",
    "- *Generating (data) distribution*: $(X,Y)\\sim P_{X,Y}$ over $\\mathcal{X}\\times\\mathcal{Y}$;\n",
    "- *Decision function*: $f\\colon\\mathcal{X}\\rightarrow\\mathcal{A}$ (i.e., *the model*); *hypothesis space* $\\mathcal{F}$ (an part of *inductive bias*);\n",
    "- *Evaluation function* (loss function): $\\mathcal{L}\\colon\\mathcal{A}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ (e.g., $\\mathcal{L}(a,y)=(a-y)^2-$squared error);\n",
    "- *The statistical risk* of a decision function (expected loss): $\\mathcal{R}(f)=\\mathbb{E}_P\\mathcal{l}(f(x),y)$;\n",
    "- *Bayes decision function*: $f^{*}=\\text{argmin}_f\\mathcal{R}(f)$ (statistical risk minimizer);\n",
    "- *Risk minimizer in $\\mathcal{F}$*: $f_{\\mathcal{F}}=\\text{argmin}_{f\\in\\mathcal{F}}\\mathcal{R}(f)=\\text{argmin}_{f\\in\\mathcal{F}}\\mathbb{E}_P\\mathcal{L}(f(X),Y)$;\n",
    "- Given data $\\mathcal{D}=\\{(x_i,y_i)\\}_{i=1}^{n}$, *empirical risk*: $\\hat{\\mathcal{R}}_{\\mathcal{D}}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(f(x_i),y_i)$;\n",
    "- *Empirical risk minimizer (ERM) in $\\mathcal{F}$*: $\\hat{f}_{\\mathcal{F},\\mathcal{D}}=\\text{argmin}_{f\\in\\mathcal{F}}\\hat{\\mathcal{R}}_{\\mathcal{D}}(f)=\\text{argmin}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(f(x_i),y_i)$;\n",
    "- *Excess risk* of a function $f$: $E(f)=\\mathcal{R}(f)-\\mathcal{R}(f^{*})$ (can $\\mathcal{E}(f)$ be negative?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Consider input/output/action spaces be $\\mathcal{X}=\\mathcal{Y}=\\mathcal{A}=\\mathbb{Z}$, generating (data) distribution $P_{X\\times Y}$ to be unifom on $\\{(-2,0),(0,0),(0,2),(2,1),(2,2)\\}$, hypothesis space $\\mathcal{F}=\\{\\alpha x+\\beta\\colon \\alpha,\\beta\\in\\mathbb{R}\\}$ of linear functions on $\\mathbb{R}$, loss function $\\mathcal{L}(a,y)=|a-y|$. What is the statistical risk minimizer $f_{\\mathcal{F}}$ in $\\mathcal{F}$ and its risk $\\mathcal{R}(f_{\\mathcal{F}})$? What $\\mathcal{F}$ will have a smaller/larger statistical risk of its $f_{\\mathcal{F}}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAp0klEQVR4nO3deXzcd33n8ddHly9Zlg/5kHVLjmxJLoXQBHqwSkm6SZZH0nazLSz3Prp+QKFQFrawZQuFLn3AoxwbGkqaEiBpaIMbKKQ0bAoUIclXfMRHbMexk9iO4kOWL0m+dMxn/5jReCSNpJ9tzfxmRu/n46GHZ77znZnPz2PPR7/f9/iYuyMiIhJEXtgBiIhI9lDSEBGRwJQ0REQkMCUNEREJTElDREQCU9IQEZHAQksaZjbbzJ4xs11mttfMPpOkj5nZV83skJntNrPXhRGriIhEFYT43leA33T3fjMrBDrN7Mfuvjmhz13AqtjPrcDXY3+KiEgIQjvT8Kj+2N3C2M/YlYb3Ao/G+m4GSs1sRTrjFBGRq8I808DM8oHtQAPwNXffMqbLSuCVhPtdsbbjSV5rHbAOYPbs2TdXVVWlJOawRSIR8vJydyhKx5fddHzZ64UXXuhx97Kp+oWaNNx9GPhlMysF/tnMWtz9uYQuluxpE7zWQ8BDAI2NjX7gwIHpDjcjtLW10draGnYYKaPjy246vuxlZkeC9MuIlOnu54A24M4xD3UBlQn3K4Bj6YlKRETGCnP2VFnsDAMzmwPcDjw/ptuTwLtis6jeAJx393GXpkREJD3CvDy1AngkNq6RB6x39x+Z2fsA3P1B4CngbuAQcBF4b1jBiohIiEnD3XcDr03S/mDCbQc+kM64RERkYhkxpiEiItlBSUNERAJT0hARkcCUNEREJDAlDRERCUxJQ0REAlPSEBGRwJQ0REQkMCUNEREJTElDREQCU9IQEZHAlDRERCQwJQ0REQlMSUNERAJT0hARkcCUNEREJDAlDRERCUxJQ0REAgstaZhZpZn93Mz2m9leM/twkj6tZnbezHbGfj4VRqwiIhIVWo1wYAj4qLvvMLP5wHYz+4m77xvTr8Pd3xJCfCIiMkZoZxruftzdd8Ru9wH7gZVhxSMiIlPLiDENM6sBXgtsSfLwG81sl5n92Mya0xuZiIgkMncPNwCzYuAXwOfc/ftjHisBIu7eb2Z3A/e7+6oJXmcdsA6grKzs5vXr16c48nD09/dTXFwcdhgpo+PLbjq+7HXbbbdtd/fXT9Uv1KRhZoXAj4Cn3f3LAfofBl7v7j2T9WtsbPQDBw5MT5AZpq2tjdbW1rDDSBkdX3bT8WUvMwuUNMKcPWXAw8D+iRKGmS2P9cPMbiEa7+n0RSkiIonCnD31a8A7gT1mtjPW9qdAFYC7PwjcB7zfzIaAS8BbPezraSIiM1hoScPdOwGbos8DwAPpiUhERKaSEbOnREQkOyhpiIhIYEoaIiISmJKGiIgEpqQhIiKBKWmIiEhgShoiIhnG3Tl06BB9fX1hhzJOmIv7REQkgbvzwgsv0N7ezrFjx7j11lu58847ww5rFCUNEZGQuTv79u2jo6ODkydPxtv37NnD7bffTkFB5nxVZ04kIiIzTCQS4bnnnqOjo4Oenqv7sBYUFFBRUcGb3/zmjEoYoKQhIpJ2w8PD7N69m87OTs6cORNvLywspK6ujqamJgYGBpgzZ06IUSanpCEikiZDQ0Ps3LmTzs5Ozp8/H28vKiqioaGB1atXU1RUBMDAwEBYYU5KSUNEJMUGBwfZvn07GzduHDUjavbs2TQ0NNDY2EhhYWGIEQanpCEikiJXrlxh27ZtbNq0iQsXLsTb58yZw0033cSqVasybsxiKtkVrYhIFrh8+TLPPPMMmzdv5tKlS/H2uXPn0tjYyKpVq8jLy85lckoaIiLT5OLFi2zevJlnnnmGK1euxNuLi4tZvXo1dXV1WZssRihpiIjcoP7+fjZt2sS2bdtGDWDPnz+fNWvWUFtbS6xyddZT0hBJtReBLwGPAZ8G7gHeAXwUqA8xLglmks+vt6yXjRs3sn37doaGhuJPWbBgAU1NTVRVVeVMshihpCGSSj8mWul+MPYD0Ad8A3gEeAK4K5zQJIAJPr9z68/ReayTna/bybAPx7svXLiQ5uZmVq5cmXPJYkRoScPMKoFHgeVABHjI3e8f08eA+4G7gYvAe9x9R7pjFbkuLxL9wrmY5LGRL6H7gN3ojCMTJfn8LkUu8cN7fsju1+wmkh8Bj7YvXryY5uZmVqxYkbPJYkSYZxpDwEfdfYeZzQe2m9lP3H1fQp+7gFWxn1uBr8f+FMl8X+Lqb6cTGQS+AjyQ+nDkGiV8fqeWnKLjTR3subgHXne1S8OhBl5T8BoK35odayymQ2hJw92PA8djt/vMbD+wEkhMGvcCj7q7A5vNrNTMVsSeK5LZHiNY0vh7lDQy0WNwYtEJOt7Uwb6mfZBwArF6/2p+o/03KD9eznDxMIc+cSi8ONPMot/HIQdhVgO0Ay3u3pvQ/iPg8+7eGbv/M+Dj7r4tyWusA9YBlJWV3bx+/fp0hJ52/f39FBcXhx1GyuTU8W0f39Rf0U9xV5Ljuzn14aRDrnx+vb29HN19lNPDp0e1L5y/kLqhOorzRx/jlZYrTLfh4WFmzZpFfn7+tL92Mrfddtt2d3/9VP1CHwg3s2Lge8AfJyaMkYeTPCVplnP3h4CHABobG721tXU6w8wYbW1t5OqxQY4d3z1EB70TtH2xjdaPtY5uLAHOkxOy/fM7evQo7e3tvPjii/E2ixhr96zlNzp+g+c+8RytH28d9Zzh4mEObZv+M42+vj5qa2tZvHjxtL/2jQg1aZhZIdGE8R13/36SLl1AZcL9CuBYOmITuWHvIDpLarJLVIXAO9MTjiTn7hw+fJj29nYOHz4cb8+L5PGaXa/h19t/nUVnFyV/boHTe8/Y33VzW5izpwx4GNjv7l+eoNuTwAfN7HGiA+DnNZ4hWeOjRKfVTpU0PpKecGQ0d+fFF1+kvb2dV155Jd6el5dHZWUlr130Wtb+1VryLk28gtsLnbPvOZuOcDNGmGcav0b0d6w9ZrYz1vanQBWAuz8IPEV0uu0hohPf3pv+MEWuUz3RdRhj5/lDNFkUxh7XdNu0cncOHDhAR0cHx45dvXCRl5dHdXU1LS0tzJs3D4Bj9x+j/MPl2KBhQ1evlnuB44XOsfuPMVg11WyH3BLm7KlOko9ZJPZx4APpiUgkBe4iug7jK0RnSUF0DOOdRM8wlDDSJhKJsH///nElVfPz86mtraWpqYm5c+eOes6FN13g8A8Ps/DbCyl5sgSIjmH03tPL2fecnXEJAzJgIFwk59UTnVL7ANBGzgx6Z4vJSqqOVMmbPXv2hM8frBqk+1PddH+qmyuHr6Rk0DubKGmISE4aHh5m165ddHZ2cvbs1XGHwsJC6uvrWbNmDbNmzQoxwuykpCEiOWVoaIhnn32WDRs2TFlSVa6dkoaI5ISJSqrOmjWLm266iZtuuilrSqpmMiUNEclquVhSNZPpb1JEstLly5fZsmULW7ZsybmSqplMSUNEsspMKKmayZQ0RCQrjJRU3bp1K4ODV9dHlJSUsHr16pwqqZrJlDREJKP19vayYcMGduzYMWNKqmYyJQ0RyUjnzp2js7OTnTt3Mjw8s0qqZjIlDRHJKKdPn6azs5Pdu3cTiUTi7SMlVcvLy0OMTpQ0RCQjnDp1io6ODp577jkSi8OVlZXR0tLCsmXLQoxORihpiEioTpw4QUdHB/v27RvVvmzZMlpaWigrKwspMklGSUNEQvHqq6/S3t7OCy+8MKp9xYoVrF27lkWLkhc+knApaYhIWiUtqWpGeXk5a9eupbS0NLzgZEpKGiKSchOVVDUzKioqaGlpYcGCBeEFKIEpaYhIyrg7hw4doqOjI2lJ1ZaWFubPnx9ihHKtlDREZNqNlFRtb2/n+PHj8fb8/Hyqq6tpbm6Ol1SV7BJq0jCzbwJvAbrdvSXJ463AD4GXY03fd/fPpi1AEbkm7s7evXtpb2+nu7s73l5QUEBNTU3SkqqSXcI+0/g20SKYj07Sp8Pd35KecETkekQiEfbs2cO2bdu4ePFivD1oSVXJHqEmDXdvN7OaMGMQkes3UUnVoqKieLJQlbzcEvaZRhBvNLNdwDHgY+6+N1knM1sHrIPoCtK2trb0RZhG/f39OXtsoOPLFpFIhOPHj/PKK6+M2p68oKCAFStWsGzZMgoKCjh27FiIUU6/K1eujJr9lUrDw8OcP3+e/Pz8tLxfUJa4XD+UAKJnGj+aYEyjBIi4e7+Z3Q3c7+6rpnrNxsZGP3DgwPQHmwHa2tpobW0NO4yU0fFltsHBQbZt28bGjRvp7++Pt8+ePZtVq1Yxe/Zs6uvrQ4wwtQ4fPkxNTU1a3quvr4/a2loWL16clvczs+3u/vqp+mX0mYa79ybcfsrM/sbMlrh7T5hxicw0V65cYevWrWzatGnUmMXYkqrp+i1cwpPRScPMlgMn3d3N7BYgDzgdclgiM8ZISdXNmzdz+fLlePu8efNobGykoaFBVfJmmLCn3P4j0AosMbMu4NNAIYC7PwjcB7zfzIaAS8BbPezraSIzwGQlVdesWUNtba2SxQwV9uypt03x+ANEp+SKSBr09/ezceNGtm3bppKqklRGX54SkfSYqKRqaWkpa9asUUlViVPSEJnBzp49y4YNG8aVVF20aBFNTU0qqSrjXFPSMLM8oDhxVpOIZJ+Rkqq7du0aVSVPJVVlKlMmDTP7B+B9wDCwHVhgZl92979KdXAiMr26u7vp7OwcV1J16dKlNDc3q6SqTCnImUaTu/ea2duBp4CPE00eShoiWeLEiRO0t7ezf//+Ue0qqSrXKkjSKDSzQuC3gQfcfdDMNO1VJAtMVFK1vLyclpYWlVSVaxYkafwtcBjYBbSbWTWgMQ2RDDZRSdWVK1fS0tKikqpy3aZMGu7+VeCrCU1HzOy21IUkItfD3Xn55Zdpb2/nyJEj8XaVVJXpFGQgfBnwl0C5u99lZk3AG4GHUx2ciExtpKRqe3s7XV1d8fa8vDyqqqpoaWmhuLg4xAgllwS5PPVt4FvAJ2P3XwC+i5KGSKhUUlXCECRpLHH39Wb2vwDcfcjMhqd6koikRiQSYd++fXR0dKikqqRdkKRxwcwWAw5gZm8Azqc0KhEZZ6SkakdHB6dPX93suaCggPr6epqampg1a1aIEcpMECRp/A/gSaDezDYAZUR3nxWRNFBJ1ZlpeHiYS5cuhR3GOEFmT+0ws/8ANAIGHHD3wSmeJiI3aGhoiB07drBhwwZ6e6/Ocp81axYNDQ2sXr2awsLCECOU6TQ4OEhPTw/d3d2cOnWK06dPs3btWioqKsIObZQgs6feNabpdWaGuz+aophEZrSBgQG2b98+YUnVxsZGCgq012i2u3LlCqdOnYr/nD17lrHlgjKxxnqQf3m/knB7NvBmYAegpCEyjaYqqXrTTTeRn58fYoRyIy5duhRPEN3d3Zw/P/HQ8KJFi6iqqmLVqlVpjDCYIJen/ijxvpktAP4+ZRGJzDCXLl3imWeeUUnVDFV4tJCF31pIyZMlHP/McRr+cwO99/Ry9r1nGaya+Er9hQsX4gni1KlT9PX1Je1nZixZsoSqqirq6uqoq6tj9uzZqTqcG3Y957gXgcxLfyJZ5uLFi2zatImtW7eqpGqGmtc+j/IPl2ODhg1F64rkX8in9J9KWfCDBRy7/xgX3nQBd6e/v39Ukrhw4ULS18zLy2Pp0qVUV1dTV1dHTU1NVk1kCDKm8S/EptsCeUATsH463tzMvgm8Beh295YkjxtwP3A30WT1HnffMR3vLRKWyUqqrlmzhpqaGhU+ygCFRwsp/3A5eZeSJO4h6FnYw7FvHWP/wH5OXjg54Uyn/Px8li9fTnV1NfX19VRVVWX1mFSQyL+YcHsIOOLuXRN1vkbfJloDfKLxkbuIntWsAm4Fvh77UyTrnD9/nkOHDrFhw4ZxJVWbmpqorKxUssggC7+1EBuMfh4Ri3By2Um6Brr47u9/l6NVR7k4Lzbu1DP6eYWFhaxYsYLq6moaGhpYuXJlTo1FBRnT+EWq3tzd282sZpIu9wKPenRKwWYzKzWzFe5+fJLniGSUs2fP0tnZyc6dO4lEIvH2RYsWxavkKVlklkgkQu/WXvbduo8j1Uc4WnWUK7OvwACwZnTfuRfnsqx5GTU1NdTX17NixYqcvqw4YdIwsz6uXpYa9RDg7l6SsqiuWgm8knC/K9Y2LmmY2TpgHUBZWRltbW1pCC/9+vv7c/bYILeO7+LFixw9epSTJ0+Oai8pKWHlypUsWLCAwcHBUTvSZrsrV65w+PDhsMO4ZpFIhP7+fnp7e+nr66Ovr4/IOyJJ+xZZEQvyF1CaX8qC/AXMnTcXqzIikQgHDx7k4MGDaY4+vSZMGu4+P52BTCDZr19JC0C5+0PAQwCNjY3e2tqawrDC09bWRq4eG+TG8XV3d9PR0cHevXvHlVQtKytj7dq1IUaXWocPH6ampibsMKY0ODjI6dOnRy2kSzwLTFR6tpTqI9VUH6nmxNtOcOef3IklfjWVMKM2Vgo8GmNmS4mu0wDA3Y+mJKLRuoDKhPsVQOatdhEBjh8/TkdHx7iSqsuXL6e5uZmysrKs/C08FwwMDIxaSHfmzJlxC+lGlJSUUFlZSUNnAzWP1VB6ujT+WNvb20YnjELgnamNPdMEmT11D/AloBzoBqqB/UBzakMDontefdDMHic6AH5e4xmSabq6uujo6FBJ1Qxy+fLlUQvpzp07N2HfhQsXUllZGV8jMX9+7CLLa4G/m+KNCoGPTFPQWSLImcZfAG8Afurur41V7XvbdLy5mf0j0AosMbMu4NNEPwbc/UHgKaLTbQ8RnXL73ul4X5HpcOTIEdrb23nppZfibSqpGo6LFy+OShKJe3WNNXYh3Zw5c5J3rAeeILo962DsZ0Rh7OeJWL8ZJEjSGHT302aWZ2Z57v5zM/vCdLy5u0+afGKzpj4wHe8lMh0mK6laWVlJS0sLJSXpmCMyc7k7Fy5ciI9HnDp1atQeXYnMjKVLl1JVVUV9fT01NTXXtn38XcBu4Ctc3QejhOglqY8w4xIGBEsa58ysGGgHvmNm3UTXa4jMGO7OwYMH6ejoUEnVNHN3+vr64kmiu7t70oV0y5YtiyeJ6urqG98JuJ7oarIHgDZm1KB3MkGSxr3AJaJ59e3AAuCzqQxKJFO4O88//zwdHR0qqZom7s758+dHJYnEbVYSFRQUxBfS1dfXU1lZmVML6TJRkKSxDvin2CrwR1Icj0hGmKqkanNz88TXwuWaRCIRzp49O2rfpsTtVRIVFRVRXl4eTxIrV67M6YV0mShI0igBnjazM8DjwBPufnKK54hkpYlKqhYWFsar5Kmk6o0ZHh7mzJkz8QTR09MzaluVRLNnz2blypXx1dbLly/X6vmQBdlG5DPAZ8zsl4DfB35hZl3ufnvKoxNJk+HhYXbu3ElnZ+eo6ZlFRUXU19ezZs2arNqJNJMMDQ2NW0g3PDyctO/cuXOpqKigpqaGVatWsXjxYiWJDHMtWy12AyeA08DS1IQjkl4qqTr9BgcHR01/PXv27ISrrefPn09FRQW1tbU0NDSwcOHCNEcr1yrI4r73Ez3DKCM6K/m/u/u+VAcmkkqTlVQdqZKXzdtXp1Ni2dKuri62bNky4Wrr0tJSKisrqampoaGhQdOTs1CQ/xXVwB+7+84UxyKScleuXIlXyRtbUrWxsZFVq1Zp9s0URsqWjlxuClK2tLa2lrq6Ok1LzgFBxjQ+kY5ARFLp0qVLbNmyhS1btqik6jUau5BusrKlI3+f2VC2VK6Pzr8lp124cIHNmzfzzDPPMDAwEG+fP38+q1evVknVMUbKliaukUg8I0uUrGzpxo0bs36XYpmckobkpL6+PjZt2qSSqlMYWUiXOHCdeCaWqKCggGXLosWG6urqsr5sqVyfIAPhHwS+4+5n0xCPyA05f/48GzZsYMeOHaOmdaqkalQkEuHcuXOjxiQSz8ASjZQtHVkjkWtlS+X6BPk1YTmw1cx2AN8EnvaJpkaIhEQlVZMbHh4etdq6p6dnwtXWs2bNory8nNra2vhCOl26k7GCDIT/bzP7M+C3iG5N/oCZrQcedvcXUx2gyGR6enro7Oxk9+7do6Z5LlmyhObmZlasWBFidOk3spBu5HJTT0/PhAvp5syZE19IV19fz9KlS2dkYpVrE+iCpLu7mZ0gurhvCFgIPGFmP3H3P0llgCLJTFZStaWlhaVLZ8b608HBQXp6euJnEmfOnJlwIV1xcXF8IV19fT2LFi1SkpBrFmRM40PAu4Ee4BvA/3T3QTPLAw4CShqSNsePH6e9vZ3nn39+VPvy5ctpaWlhyZIlIUWWHollS0dWW090tXjBggVUVlbG10ioKJRMhyBnGkuA33X3I4mN7h4xs7ekJiyR0bq6umhvb+fgwYOj2svLy1m7dm3Obj8xUrZ0ZNB6qrKliQvp4mVLRaZRkDGNT03y2P7pDUdktJlWUvXixYujFtJNVLbUzFi8eDHV1dXxJKGt2iUdQp1kbWZ3AvcD+cA33P3zYx5vBX4IvBxr+r67qwBUjnN3XnrpJdrb2zl69Gi8PS8vj4qKipwpqTq2bGl3dzcXLlxI2jcvL4+ysrJRC+m0RbuEIbSkYWb5wNeAO4AuotN6n0yyGWKHu+sy2Azg7pw+fZqHH36YV199Nd6eKyVV3Z1Lly5x6NChwGVLR4oNVVVVabddyQhhnmncAhxy95cAzOxxoqVltYPuDDNSUrW9vZ0TJ07E27O9pKq7j1tIF6RsaUNDAxUVFVpIJxkpzKSxEngl4X4XcGuSfm80s13AMeBj7r432YuZ2TqipWkpKyujra1teqPNEP39/TlzbO7OqVOnOHr06KjLMvn5+SxdupQVK1ZQVFQUv76f6SKRCBcvXqS3t5fe3l76+vomXCNRUFBASUkJpaWllJaWUlxcHJ/++vLLL/Pyyy8nfV6my6V/n8nk+vEFEWbSSDZBfOzcwR1Atbv3m9ndwA+AVclezN0fAh4CaGxs9FzdNK2trS3rN4QbHh5mz549dHZ2jiupunTpUm655ZasuF4/PDw8biHdVGVLAW6//XaWLVuWk2skcuHf52Ry/fiCCDNpdAGVCfcriJ5NxLl7b8Ltp8zsb8xsibv3pClGmUZDQ0Ps2rVr0pKqx44dy9iEMTQ0NGoh3enTpydcSDdv3rz4auuGhoZ42dK2tjaWL1+e5shFpk+YSWMrsMrMaoFXgbcC/zWxg5ktB07GVqTfAuQRLTcrWWRwcJBnn30260qqDgwMjFttPdFCuvnz58cX0tXX1+fsuhGR0JKGuw/FdtB9muiU22+6+14ze1/s8QeB+4D3m9kQcAl4qzZLzB4DAwNs27aNTZs2ZUVJ1ZGypYkL6aYqWzpSbCgXpgCLBBHq/1h3fwp4akzbgwm3HwAeSHdccmMmKqk6d+5cbrrppowpqXotZUsXL14cX21dX1/P3Llz0xipSObInF/zJOtNVlJ19erV1NfXh7rV9rWULS0rK6Oqqoq6ujpqa2tVtlQkRklDbtiFCxfYtGkTW7duHVdSdaRKXrqThbvT19c36kziWsqWFhUVpTVekWyhpCHXra+vj40bN7J9+/ZRhX0WLFjAmjVrqK6uTtu00sSypSNJYrKypcuXL4+vtq6srMyosRWRTKb/KXLNJiqpunDhQtasWZOWkqrXWra0vLw8niRUtlTk+ilpSGBnz56lo6ODXbt2jVqfsHjxYpqamlJaUnWkbGnimMREC+lmzZrFypUr4xXpVLZUZPooaciUwiipmli2dGQhXZCypQ0NDZSVleXkamuRTKCkIRM6efJkvKRqomXLltHc3DytJVVHypYePXqUQ4cOTVm2tLKyMp4kFi5cqCQhkiZKGjJOOkqqjpQtHbncFLRsaX19PQsWLLjh9xeR66OkIXGpLKl6rWVLq6ur42MS2VxDQyTXKGkIhw8fpr29fdR23CMlVdeuXXtdv9knli3t7u6edCFdYtnSY8eOcccdd1z3sYhIailpzFCTlVStrKykpaWF+fPnB36t/v7+UWcSQcuW1tbWjlpIlw11M0RmMiWNGcbdOXjwIO3t7eNKqo5UyZvqcpC709vbOypJTFa2dPny5VRVValsqUgOUNKYIdyd/fv309HRMa6kak1NDc3NzRNuwheJRDh//vyoNRITlS0tLCyMly2tr69X2VKRHKOkkeMikQh79+6lo6Nj1KWfgoICamtraWpqYs6cOeOeM3YhXeI2IYmKioooLy+ntraWuro6ysvLtZBOJIcpaeSokZKqHR0dnDlzJt5eWFhIXV0dTU1N8Qp5iWVLRxbSTVa2dGQhXX19fc6WLRWR5JQ0cszQ0BA7d+5kw4YNE5ZUzcvLo6enJ34mEaRs6cgaiZGypSIyMylpZIMXgS8BjwGfBu4B3gF8FKiPdhkcHGTHjh1s3LhxXEnVuro6Fi5cyJkzZ/jFL34xadnSkpISKioq4hXpVLZ0GgT4/ESyhZJGpvsx0aK3g7EfgD7gG8AjMPDdAbYt2sbGjRtHTXMtKiqipKSEwcFB9u/fP+HLl5aWxivSqWxpCkzx+fEEcFc4oYlcj1CThpndCdxPtEb4N9z982Met9jjdwMXgfe4+460BxqWF4l+4SSpHXQ57zJbb97Kpo5NXJp7dbqrmeHuDAwM0NPTM+55I2VLR84kVLY0hSb5/OJJ5D5gNzrjkKwRWtIws3zga8AdQBew1cyedPd9Cd3uAlbFfm4Fvh77c2b4Eld/O40ZiAzw4zt/zLOve5bBovEzmhIvOyWWLa2vr6e2tjY++C1pkOTzG2cQ+ArwQOrDEZkOYZ5p3AIccveXAMzsceBeIDFp3As86tFvws1mVmpmK9z9ePrDDcFjxL90eot7eeydj3Hq4il4Q/LueXl5LFu2bFTZUi2kC1HC5zehQeDvUdKQrBFm0lgJvJJwv4vxZxHJ+qwExiUNM1sHrAMoKyujra1tOmMNx6ev3hyMDEYTRoI88ijJK6G0oJTSplLmz58fXyPx6quvjlrxnS36+/tz47ODUZ/fiP6Kftq+2Db+gSRN2SinPr8kcv34gggzaSSbtzl2Sk+QPtFG94eAhwAaGxu9tbX1hoLLCPcQHTSNeXndy5wqP8XabWt53Y7XUX6inLxIHpQA58MKcnq1tbWRE58djPv8ANq+2Ebrx1pHN+rzyxq5fnxBhLl0twuoTLhfARy7jj656x1AwtWlP/jGH/Cr836Ve350DxXHKqIJoxB4Z1gByqTGfH5J6fOTLBNm0tgKrDKzWjMrAt4KPDmmz5PAuyzqDcD5GTOeAdF5/AlfOnmRvPEL6wqBj6QzKAlszOeXlD4/yTKhJQ13HwI+CDwN7AfWu/teM3ufmb0v1u0p4CXgEPB3wB+GEmxY6onO45/L+C+fwlj7E2i6ZqbS5yc5KNR1Gu7+FNHEkNj2YMJtBz6Q7rgyyl1E5/F/hegsG4heA38n0d9Q9YWT2fT5SY7RivBsUE90SuYDRGfZ5Mig6Yyhz09yiPawFhGRwJQ0REQkMCUNEREJTElDREQCU9IQEZHAlDRERCQwJQ0REQlMSUNERAJT0hARkcCUNEREJDAlDRERCUxJQ0REAlPSEBGRwJQ0REQkMCUNEREJTElDREQCU9IQEZHAQqncZ2aLgO8CNcBh4Pfc/WySfoeBPmAYGHL316cvShERGSusM41PAD9z91XAz2L3J3Kbu/+yEoaISPjCShr3Ao/Ebj8C/HZIcYiIyDUIK2ksc/fjALE/l07Qz4F/M7PtZrYubdGJiEhS5u6peWGznwLLkzz0SeARdy9N6HvW3RcmeY1ydz9mZkuBnwB/5O7tE7zfOmAdQFlZ2c3r16+fhqPIPP39/RQXF4cdRsro+LKbji973XbbbduDDAOkLGlM+qZmB4BWdz9uZiuANndvnOI5fw70u/sXp3r9xsZGP3DgwPQEm2Ha2tpobW0NO4yU0fFlNx1f9jKzQEkjrMtTTwLvjt1+N/DDsR3MbJ6ZzR+5DfwW8FzaIhQRkXHCShqfB+4ws4PAHbH7mFm5mT0V67MM6DSzXcAzwL+6+/8LJVoREQFCWqfh7qeBNydpPwbcHbv9EvCaNIcmIiKT0IpwEREJTElDREQCU9IQEZHAlDRERCQwJQ0REQlMSUNERAJT0hARkcCUNEREJDAlDRERCUxJQ0REAlPSEBGRwJQ0REQkMCUNEREJTElDREQCU9IQEZHAlDRERCQwJQ0REQlMSUNERAJT0hARkcBCSRpm9l/MbK+ZRczs9ZP0u9PMDpjZITP7RDpjFBGR8cI603gO+F2gfaIOZpYPfA24C2gC3mZmTekJT0REkikI403dfT+AmU3W7RbgkLu/FOv7OHAvsC/lAYqISFKhJI2AVgKvJNzvAm6dqLOZrQPWxe5eMbPnUhhbmJYAPWEHkUI6vuym48tejUE6pSxpmNlPgeVJHvqku/8wyEskafOJOrv7Q8BDsffe5u4TjpVks1w+NtDxZTsdX/Yys21B+qUsabj77Tf4El1AZcL9CuDYDb6miIjcgEyecrsVWGVmtWZWBLwVeDLkmEREZrSwptz+jpl1AW8E/tXMno61l5vZUwDuPgR8EHga2A+sd/e9Ad/ioRSEnSly+dhAx5ftdHzZK9CxmfuEwwQiIiKjZPLlKRERyTBKGiIiElhOJg0z+wsz221mO83s38ysPOyYppOZ/ZWZPR87xn82s9KwY5pOQbeZySa5viWOmX3TzLpzcX2UmVWa2c/NbH/s3+WHw45pOpnZbDN7xsx2xY7vM5P2z8UxDTMrcffe2O0PAU3u/r6Qw5o2ZvZbwL+7+5CZfQHA3T8ecljTxszWABHgb4GPuXug+eOZKrYlzgvAHUSnkm8F3ubuObO7gZm9CegHHnX3lrDjmU5mtgJY4e47zGw+sB347Vz5/Cy6Ncc8d+83s0KgE/iwu29O1j8nzzRGEkbMPCZZFJiN3P3fYrPLADYTXcOSM9x9v7sfCDuOaRTfEsfdB4CRLXFyhru3A2fCjiMV3P24u++I3e4jOptzZbhRTR+P6o/dLYz9TPidmZNJA8DMPmdmrwBvBz4Vdjwp9N+AH4cdhEwq2ZY4OfOlM5OYWQ3wWmBLyKFMKzPLN7OdQDfwE3ef8PiyNmmY2U/N7LkkP/cCuPsn3b0S+A7R9R5ZZarji/X5JDBE9BizSpDjyyHXtCWOZCYzKwa+B/zxmKsZWc/dh939l4letbjFzCa8xJjJGxZO6hq2KfkH4F+BT6cwnGk31fGZ2buBtwBv9iwcmJqGbWayibbEyXKxa/3fA77j7t8PO55UcfdzZtYG3Em0hMU4WXumMRkzW5Vw9x7g+bBiSQUzuxP4OHCPu18MOx6ZkrbEyWKxgeKHgf3u/uWw45luZlY2MgPTzOYAtzPJd2auzp76HtFtfiPAEeB97v5quFFNHzM7BMwCTseaNufY7LDfAf4aKAPOATvd/T+GGtQNMrO7gf8L5APfdPfPhRvR9DKzfwRaiW4dfhL4tLs/HGpQ08TMfh3oAPYQ/U4B+FN3fyq8qKaPmf0S8AjRf5t5RLds+uyE/XMxaYiISGrk5OUpERFJDSUNEREJTElDREQCU9IQEZHAlDRERCQwJQ2RNDOz/ql7iWQmJQ0REQlMSUNkAmb2K7GaJbPNbF6s1kDLmD5fMLM/TLj/52b2UTMrNrOfmdkOM9uTbE8tM2s1sx8l3H/AzN4Tu32zmf3CzLab2dOx7bkxsw+Z2b5YXI+n7OBFJpC1e0+JpJq7bzWzJ4H/A8wBHnP3sfvxPE50pfffxO7/HtF9ey4Dv+PuvWa2BNhsZk8G2Scsts/RXwP3uvspM/t94HNEdzT+BFDr7ldyrfiWZAclDZHJfZbo3lGXgQ+NfdDdnzWzpbHqkGXAWXc/Gvvi/8tYcaII0a3QlwEnArxnI9AC/CS67RH5wPHYY7uB75jZD4Af3MBxiVwXJQ2RyS0CiokWppkNXEjS5wngPmA50TMPiNZxKQNudvdBMzsce36iIUZfIh553IC97v7GJO/1n4A3Ed2I88/MrDmhIJdIymlMQ2RyDwF/RrRmyRcm6PM40Z1r7yOaQAAWAN2xhHEbUJ3keUeAJjObZWYLgDfH2g8AZWb2RoherjKzZjPLAyrd/efAnwClRBOaSNroTENkAmb2LmDI3f8hVud7o5n9prv/e2I/d98bqx39qruPXEb6DvAvZrYN2EmSrabd/RUzW0/0ktNB4NlY+4CZ3Qd8NZZMCoiOm7wAPBZrM+Ar7n5uuo9bZDLa5VZERALT5SkREQlMSUNERAJT0hARkcCUNEREJDAlDRERCUxJQ0REAlPSEBGRwP4/q46ng488WbsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([-2,2.5],[0,1.125],linestyle='-',color='grey',lw=2.5)\n",
    "plt.plot([-2,2.5],[0,2.25],linestyle='-',color='grey',lw=2.5)\n",
    "plt.fill_between(x=[-2,2.5],y1=[0,1.125],y2=[0,2.25],alpha=0.3,color=\"grey\")\n",
    "plt.scatter([-2,0,0,2,2],[0,0,2,1,2],c=\"fuchsia\",s=100)\n",
    "\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.ylim((-1,3))\n",
    "plt.xlim((-3,3))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "#### Excess Risk Decomposition\n",
    "The excess risk of an ERM minimizer in $\\mathcal{F}$ given data $\\mathcal{D}$ can be decomposed: $E(\\hat{f}_{\\mathcal{F},\\mathcal{D}})=\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f^{*})=\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f_{\\mathcal{F}})+\\mathcal{R}(f_{\\mathcal{F}})-\\mathcal{R}(f^{*})$.\n",
    "- *Estimation error*: $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}=\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f_{\\mathcal{F}})$ (is it always non-positive, always non-negative, or neither?);\n",
    "- *Approximation error*: $\\mathcal{E}_{\\mathcal{F}}=\\mathcal{R}(f_{\\mathcal{F}})-\\mathcal{R}(f^{*})$ (is it always non-positive, always non-negative, or neither?),\n",
    "\n",
    "so that $E(\\hat{f}_{\\mathcal{F},\\mathcal{D}})=\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}+\\mathcal{E}_{\\mathcal{F}}$. The modelling task in a nutshell: (a) Choose a loss function $\\mathcal{L}$, (b) choose a hypothesis space $\\mathcal{F}$, (c) optimize the empirical risk $\\mathcal{\\hat{R}}_{\\mathcal{D}}(f)$ over $\\mathcal{F}$. Do you always expect (or want) to get an ERM minimizer $\\hat{f}_{\\mathcal{F},\\mathcal{D}}$ as a result? Why or why not?\n",
    "\n",
    "In practice, we will get some $\\tilde{f}_{\\mathcal{F},\\mathcal{D}}$ as a result of optimization. We thus can define *optimization error* $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}=\\mathcal{R}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\mathcal{R}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})$. Hence the excess risk of the resulting model $\\tilde{f}_{\\mathcal{F},\\mathcal{D}}$ is\n",
    "\n",
    "$$E(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})=\\mathcal{R}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\mathcal{R}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})+\\mathcal{R}(\\hat{f}_{\\mathcal{\\mathcal{F},D}})-\\mathcal{R}(f_{\\mathcal{F}})+\\mathcal{R}(f_{\\mathcal{F}})-\\mathcal{R}(f^{*})=\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}+\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}+\\mathcal{E}_{\\mathcal{F}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand $\\tilde{f}_{\\mathcal{F},\\mathcal{D}}$ and optimization error $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}$ better:\n",
    "1. Is optimization error $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}}^{(o)}=\\mathcal{R}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\mathcal{R}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})$ always non-positive, always non-negative, or neither?\n",
    "2. Is quantity $\\hat{\\mathcal{R}}_{\\mathcal{D}}(\\tilde{f}_{\\mathcal{F},\\mathcal{D}})-\\hat{\\mathcal{R}}_{\\mathcal{D}}(\\hat{f}_{\\mathcal{F},\\mathcal{D}})$ always non-positive, always non-negative, or neither?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Exercises\n",
    "1. Let $\\mathcal{F}_1\\subseteq\\mathcal{F}_2$ be two hypothesis spaces. Is the difference of approximation errors $\\mathcal{E}_{\\mathcal{F}_1}-\\mathcal{E}_{\\mathcal{F}_2}$ always non-positive, always non-negative, or neither?\n",
    "2. Let $\\mathcal{F}$ be some hypothesis space and $\\mathcal{D}_1,\\mathcal{D}_2$ be two datasets generated by $P_{X\\times Y}$ with $|\\mathcal{D}_1|<|\\mathcal{D}_2|$. Is $\\mathcal{E}_{\\mathcal{F},\\mathcal{D}_1}-\\mathcal{E}_{\\mathcal{F},\\mathcal{D}_2}$ always non-positive, always non-negative, or neither? What about $\\mathcal{E}^{(o)}_{\\mathcal{F},\\mathcal{D}_1}-\\mathcal{E}^{(o)}_{\\mathcal{F},\\mathcal{D}_2}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation\n",
    "From a probabilistic perspective, logistic regression is a parametric predictive model $Y\\sim Bernoulli(\\sigma(\\theta^T X))$, where $\\sigma=(1+e^{-\\theta^T X})^{-1}$ is a logistic function applied to $\\theta^T X$ where $\\theta$ is a parameter vector of the model. \n",
    "\n",
    "#### Theory of fitting\n",
    "The likelihood $\\mathcal{L}$ and negative log-likelihood $l$ functions of this model are\n",
    "\n",
    "$$\\mathcal{L}(\\theta|Y)=p(y|X,\\theta)=\\prod_{i=1}^{n}\\sigma\\left(\\theta^T \\bar{x_i}^T\\right)^{y_i}\\left(1-\\sigma\\left(\\theta^T\\bar{x_i}^T\\right)\\right)^{1-y_i},$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}y_i\\log\\sigma\\left(\\theta^T\\bar{x}_i^T\\right)+(1-y_i)\\log\\left(1-\\sigma\\left(\\theta^T \\bar{x}_i^T\\right)\\right),$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}y_i\\log\\left(1+e^{\\theta^T\\bar{x}_i^T}\\right)^{-1}+(1-y_i)\\log\\left(\\frac{e^{\\theta^T \\bar{x}_i^T}}{1+e^{\\theta^T\\bar{x}_i^T}}\\right),$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}-y_i\\log\\left(1+e^{\\theta^T\\bar{x}_i^T}\\right)+\\theta^T\\bar{x}_i^T-y_i\\theta^T\\bar{x}_i^T-\\log(1+e^{\\theta^T\\bar{x}_i^T})+y_i\\log\\left(1+e^{\\theta^T\\bar{x}_i^T}\\right)$$\n",
    "$$l(\\theta)=-\\sum_{i=1}^{n}\\theta^T\\bar{x}_i^T-y_i\\theta^T\\bar{x}_i^T-\\log(1+e^{\\theta^T\\bar{x}_i^T})$$\n",
    "\n",
    "\n",
    "The loss fuction above is called *binary cross-entropy*; it frequently appears in the classification setting. We will use gradient descent on the likelihood function to obtain an estimate $\\hat{\\theta}$. Therefore, we need to compute partial derivatives\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial\\theta_j}=-\\sum_{i=1}^{n}x_{ij}-y_{i}x_{ij}-\\frac{x_{ij}e^{\\theta^T\\bar{x}_i^T}}{1+e^{\\theta^T\\bar{x}_i^T}}=-\\sum_{i=1}^{n}x_{ij}\\left(-y_{i}+\\sigma\\left(\\theta^T\\bar{x}_i^T\\right)\\right)=\\sum_{i=1}^{n}x_{ij}y_{i}-x_{ij}\\sigma\\left(\\theta^T\\bar{x}_i^T\\right)$$\n",
    "\n",
    "Now, let's fugure out the update rules. Letting $\\alpha$ be the learning rate, we have\n",
    "\n",
    "$$\\theta^{(t+1)}_j=\\theta^{(t)}_j-\\alpha\\frac{\\partial l}{\\partial\\theta_{j}}\\biggr|_{\\theta=\\theta^{(t)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory of predicting\n",
    "Given input vector $\\bar{x}$, the output of logistic regression is a number $\\sigma\\left(\\theta\n",
    "^T\\bar{x}\\right)\\in[0,1]$. To convert it into a prediction $y\\in\\{0,1\\}$, we need to agree on the mapping $\\sigma\\left(\\theta^T\\bar{x}\\right)\\mapsto y$. By default, the threshold value is naturally set to $\\tau=0.5$, i.e. $y=\\mathbb{1}_{\\sigma\\left(\\theta^T\\bar{x}\\right)\\geq 0.5}$. The choice of $\\tau$ affects the trade-off between type I/II errors; $\\tau=0.5$ is the unbiased choice, where we don't differentiate between error types. However, in some applications, costs associated with type I/II errors might be surprisingly different (can you think of such scenarios)?  Therefore, it is common to consider other [biased] threshold values. In fact, one common measure of model's performance is *AUC ROC (Area Under the Receiver Operating Curve)*, which evaluates the model at all thresholds simultaneously (more later in this course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "Disclaimer: we will write the most basic Logistic Regression class that only supports gradient descent (not stochastic), only one initialization scheme, no regularization or data randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# We will reuse the voting data from last time:\n",
    "train=pd.read_csv('clean_train.csv',sep=',',index_col=0)\n",
    "test=pd.read_csv('clean_test.csv',sep=',',index_col=0)\n",
    "train_X=train.iloc[:,:-1]\n",
    "train_y=train.iloc[:,-1].astype(int)\n",
    "test_X=test.iloc[:,:-1]\n",
    "test_y=pd.DataFrame(test.iloc[:,-1].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to avoid sigmoid saturation:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "train_X=scaler.fit_transform(train_X)\n",
    "test_X=scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions:\n",
    "def sigmoid(t):\n",
    "    return 1./(1+math.e**t)\n",
    "\n",
    "def compute_loss(X,y,theta):\n",
    "    return -sum([y_i*np.log(sigmoid(np.dot(theta,X_i)))+(1-y_i)*np.log(1-sigmoid(np.dot(theta,X_i))) for X_i,y_i in zip(X,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LogisticRegression class:\n",
    "class LogisticRegression():\n",
    "    def __init__(self,alpha,max_iter,tolerance=1e-5):\n",
    "        self.alpha=alpha\n",
    "        self.tolerance=tolerance\n",
    "        self.max_iter=max_iter\n",
    "    def gradient(self,X,y,theta):\n",
    "        return [sum([X_i[j]*y_i-X_i[j]*sigmoid(np.dot(X_i,theta)) for X_i,y_i in zip(X,y)]) for j in range(X.shape[1])]\n",
    "    def fit(self,X,y):\n",
    "        progress=[]\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        self.theta=np.random.normal(loc=0,scale=1,size=X.shape[1])\n",
    "        loss=compute_loss(X,y,self.theta)\n",
    "        while self.max_iter>0 and loss>self.tolerance:\n",
    "            if self.max_iter%50==0:\n",
    "                progress.append(loss)\n",
    "            self.theta-=self.alpha*np.array(self.gradient(X,y,self.theta))\n",
    "            loss=compute_loss(X,y,self.theta)\n",
    "            self.max_iter-=1\n",
    "        return progress\n",
    "    def predict(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [1 if sigmoid(np.dot(self.theta,X_i))>0.5 else 0 for X_i in X]\n",
    "    def proba(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [sigmoid(np.dot(self.theta,X_i)) for X_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model:\n",
    "iterations=10000\n",
    "model=LogisticRegression(0.01,iterations)\n",
    "progress=model.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training:\n",
    "plt.plot(range(iterations)[::50],progress,color='fuchsia',label=\"training loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch predictions:\n",
    "test_y[\"predicted\"]=model.predict(test_X)\n",
    "test_y[\"correct?\"]=(test_y[\"predicted\"]==test_y[\"trumpWinner\"]).astype(int)\n",
    "test_y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we assessed the difficulty of a particular state (observation) by calculating the variance of the associated predictions across all learners in the ensemble. Can we get similar insights from our logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual \"probabilities\" (sigmoid output):\n",
    "test_y=pd.DataFrame(test_y,dtype=object) # otherwise Pandas will convert all ints to floats;\n",
    "test_y[\"probability\"]=[round(p,2) for p in model.proba(test_X)]\n",
    "test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to sklearn's implementation:\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "model_2=LR(penalty='none',max_iter=iterations)\n",
    "model_2.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y=test_y.drop(\"probability\",axis=1)\n",
    "test_y[\"predicted (sk)\"]=model_2.predict(test_X)\n",
    "test_y[\"correct (sk)?\"]=(test_y[\"predicted (sk)\"]==test_y[\"trumpWinner\"]).astype(int)\n",
    "test_y.T"
   ]
  }
 ],
 "metadata": {
  "direction": "ltr",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
