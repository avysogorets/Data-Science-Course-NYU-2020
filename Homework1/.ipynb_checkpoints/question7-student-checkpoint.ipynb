{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary code:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train=pd.read_csv('train_elections.csv',index_col=0)\n",
    "test=pd.read_csv('test_elections.csv',index_col=0)\n",
    "features=train.columns[:-2]\n",
    "target=\"trumpWinner\"\n",
    "scaler=MinMaxScaler()\n",
    "train_X=train.loc[:,features]\n",
    "train_y=train.loc[:,target].values.astype(int)\n",
    "test_X=test.loc[:,features]\n",
    "test_y=test.loc[:,target].values.astype(int)\n",
    "train_X=scaler.fit_transform(train_X)\n",
    "test_X=scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions:\n",
    "def sigmoid(t):\n",
    "    return 1./(1+np.exp(-t))\n",
    "def compute_loss(X,y,theta):\n",
    "    return -sum([y_i*np.log(sigmoid(np.dot(theta,X_i)))+(1-y_i)*np.log(1-sigmoid(np.dot(theta,X_i))) for X_i,y_i in zip(X,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Logistic Regression class from Lab3:\n",
    "class LogisticRegression():\n",
    "    def __init__(self,alpha,max_iter,tolerance=1e-5):\n",
    "        self.alpha=alpha\n",
    "        self.tolerance=tolerance\n",
    "        self.max_iter=max_iter\n",
    "    def gradient(self,X,y,theta):\n",
    "        return [-sum([X_i[j]*y_i-X_i[j]*sigmoid(np.dot(X_i,theta)) for X_i,y_i in zip(X,y)]) for j in range(X.shape[1])]\n",
    "    def fit(self,X,y):\n",
    "        progress=[]\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        self.theta=np.random.normal(loc=0,scale=1,size=X.shape[1])\n",
    "        loss=compute_loss(X,y,self.theta)\n",
    "        while self.max_iter>0:\n",
    "            if self.max_iter%500==0 and loss>self.tolerance:\n",
    "                loss=compute_loss(X,y,self.theta)\n",
    "                progress.append(loss)\n",
    "            self.theta-=self.alpha*np.array(self.gradient(X,y,self.theta))\n",
    "            self.max_iter-=1\n",
    "        return progress\n",
    "    def predict(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [1 if sigmoid(np.dot(self.theta,X_i))>0.5 else 0 for X_i in X]\n",
    "    def proba(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [sigmoid(np.dot(self.theta,X_i)) for X_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (b):\n",
    "'''\n",
    "Complete the 'gradient(self,X,y,theta)' method with computations performed\n",
    "in the matrix form. \n",
    "- arguments: (n,d) matrix X, (1,n)-vector y, and a parameter (1,d+1)-vector theta. \n",
    "- returns: (1,d+1)-vector with gradients of the loss l(theta|X,y) with respect to theta.\n",
    "Note that the 'fit' method of FasterLogisticRegression converts given X and y to the \n",
    "required format. The rest of the code is slightly modified from the LogisticRegression\n",
    "class to account for 2-dimensional shapes of theta and y. Otherwise, the two classes are identical.\n",
    "'''\n",
    "\n",
    "class FasterLogisticRegression():\n",
    "    def __init__(self,alpha,max_iter,tolerance=1e-5):\n",
    "        self.alpha=alpha\n",
    "        self.tolerance=tolerance\n",
    "        self.max_iter=max_iter\n",
    "    def gradient(self,X,y,theta):\n",
    "        \n",
    "        # TO DO\n",
    "        \n",
    "        return None\n",
    "    def fit(self,X,y):\n",
    "        progress=[]\n",
    "        y=y.reshape((1,X.shape[0]))\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        self.theta=np.random.normal(loc=0,scale=1,size=X.shape[1]).reshape((1,X.shape[1]))\n",
    "        loss=compute_loss(X,y[0],self.theta[0])\n",
    "        while self.max_iter>=0 and loss>self.tolerance:\n",
    "            if self.max_iter%500==0:\n",
    "                loss=compute_loss(X,y[0],self.theta[0])\n",
    "                progress.append(loss)\n",
    "            self.theta-=self.alpha*self.gradient(X,y,self.theta)\n",
    "            self.max_iter-=1\n",
    "        return progress\n",
    "    def predict(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [1 if sigmoid(np.dot(self.theta,X_i))>0.5 else 0 for X_i in X]\n",
    "    def proba(self,X):\n",
    "        X=np.hstack([X,np.ones((X.shape[0],1))])\n",
    "        return [sigmoid(np.dot(self.theta,X_i)) for X_i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fit both LogisticRegression and FasterLogisticRegression with the learning\n",
    "rate of 0.01 and max_iter of 10,000. Time both models and report their\n",
    "training times. Comment on your findings.\n",
    "'''\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (c):\n",
    "\n",
    "'''\n",
    "Identify features corresponding to the two largest magnitude coefficients\n",
    "in your model.\n",
    "'''\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project the training data onto these two features and produce a scatter\n",
    "plot. Further, draw a projection of the decision boundary onto this plane.\n",
    "Color points corresponding to positive target values in red and\n",
    "in blue otherwise. Do not acount for the intercept coefficient.\n",
    "'''\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (d):\n",
    "\n",
    "'''\n",
    "Fetch test predictions from your logistic model and construct a table\n",
    "with the following rows: (a) ground truth, (b) predicted value,\n",
    "(c) prediction correct? Compare this to your results with trees and\n",
    "random forest in Question 4. Do mistakes of the logistic regression\n",
    "model correlate with difficulty (uncertainty) identified by the random forest?\n",
    "'''\n",
    "\n",
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
